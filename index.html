<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" type="image/x-icon" href="https://github.com/XU-TIANYANG/XU-TIANYANG.github.io/blob/main/img/img_ico.ico?">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="/img/img_ico.ico" />
<link rel="bookmark" href="/img/img_ico.ico" type="image/x-icon"　/>
<title>Tianyang Xu 徐天阳</title>
</head>
<body>
<td id="layout-content">
<div id="toptitle">
<h1>Tianyang Xu 徐天阳</h1>
</div>
<table class="imgtable"><tr><td>
<a href="XU-TIANYANG.github.io/"><img src="/img/img_tianyang.jpg" alt="alt text" height="180px" /></a>&nbsp;</td>  
<td align="left"><p></b>Tianyang Xu</b> <br />
Associate Professor<br />
School of Artificial Intelligence and Computer Science<br />
Jiangnan University <br />
Wuxi, China <br />
E-mail: tianyang_xu@163.com<br />
<div class="menu-item"><a href="https://scholar.google.com/citations?user=bbSNrAgAAAAJ&hl=en">Google Scholar</a></div>
<div class="menu-item"><a href="http://ai.jiangnan.edu.cn/info/1014/2040.htm">Institution Page</a></div>
</td></tr></table>
<p>I am currently an Associate Professor with the School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China. I received my Ph.D. degree from Jiangnan University in 2019 under the supervision of Prof. Xiao-Jun Wu and Prof. Josef Kittler, and Bachelor degree from Nanjing University in 2011. I was a research fellow at the Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, United Kingdom, from 2019 to 2021. My research interests include computer vision and pattern recognition, e.g., visual tracking, video classification, multimodal analysis, and manifold learning.</p>
<h3><font color=blue> Publications </font></h3>
<ul>
</li>
<li><p>Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking. <b>ACM MM</b>, 2025. </p> 
</li>
<li><p>Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action Recognition with Virtual Connections. <b>ICCV</b>, 2025. </p> 
</li>
<li><p>DFL-Net: Disentangled Feature Learning Network for Multi-view Clustering. <b>IEEE TKDE</b>, 2025. </p> 
</li>
<li><p>Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning. <b>ICML</b>, 2025. [<a href="https://arxiv.org/abs/2505.21877"><font color=blue>paper</font></a>] [<a href="https://github.com/Hongyao-Chen/HybridBN"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion. <b>CVPR</b>, 2025. [<a href="https://arxiv.org/abs/2502.19854"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/GIFNet"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>Learning Structure-Supporting Dependencies via Keypoint Interactive Transformer for General Mammal Pose Estimation. <b>IJCV</b>, 2025. [<a href="https://link.springer.com/article/10.1007/s11263-025-02355-0"><font color=blue>paper</font></a>] [<a href="https://github.com/Raojiyong/KITPose"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>R-DTI: Drug Target Interaction Prediction based on Second-Order Relevance Exploration. <b>AAAI</b>, 2025.  [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/33909"><font color=blue>paper</font></a>] </p> 
</li>
<li><p>FusionBooster: A Unified Image Fusion Boosting Paradigm. <b>IJCV</b>, 2024. [<a href="https://arxiv.org/abs/2305.05970"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/FusionBooster"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>Adaptive Log-Euclidean Metrics for SPD Matrix Learning. <b>IEEE TIP</b>, 2024. [<a href="https://ieeexplore.ieee.org/document/10681034"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>APMG: 3D Molecule Generation Driven by Atomic Chemical Properties. <b>IEEE/ACM TCBB</b>, 2024. [<a href="https://pubmed.ncbi.nlm.nih.gov/39255082/"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>M-Adapter: Multi-Level Image-to-Video Adaptation for Video Action Recognition. <b>CVIU</b>, 2024. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314224002315"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>MMDRFuse: Distilled Mini-Model with Dynamic Refresh for Multi-Modality Image Fusion. <b>ACM MM</b>, 2024. (<font color=blue><b>oral</b></font>) [<a href="https://arxiv.org/abs/2408.15641"><font color=blue>paper</font></a>] [<a href="https://github.com/yanglinDeng/MMDRFuse"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>Learning Feature Restoration Transformer for Robust Dehazing Visual Object Tracking. <b>IJCV</b>, 2024. [<a href="https://link.springer.com/article/10.1007/s11263-024-02182-9"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>View-shuffled clustering via the modified Hungarian algorithm. <b>Neural Networks</b>, 2024. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608024005264"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>HabLSTM: A Non-stationary Feature Focusing LSTM for Spatiotemporal Prediction of Harmful Algal Bloom. <b>IEEE TGRS</b>, 2024. [<a href="https://www.scilit.net/publications/51e867b49ad03a9fe009baa4a6350de4"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>Multi-Level Fusion for Robust RGBT Tracking via Enhanced Thermal Representation. <b>ACM TOMM</b>, 2024. [<a href="https://dl.acm.org/doi/10.1145/3678176"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>Efficient Few-Shot Action Recognition via Multi-Level Post-Reasoning. <b>ECCV</b>, 2024. [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00305.pdf"><font color=blue>paper</font></a>] [<a href="https://github.com/cong-wu/EMP-Net"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>C2C: Component-to-Composition Learning for Zero-Shot Compositional Action Recognitionn. <b>ECCV</b>, 2024. (<font color=blue><b>oral</b></font>) [<a href="https://arxiv.org/pdf/2407.06113"><font color=blue>paper</font></a>] [<a href="https://github.com/RongchangLi/ZSCAR_C2C"><font color=blue>Code</font></a>]</p> 
</li>
<li><p>YOLO-WIT: 红外遥感弱小船舶目标检测算法. <b>北京航空航天大学学报</b>, 2024. [<a href="https://kns.cnki.net/kcms2/article/abstract?v=dMo7FjsfBlEhjRp0ffdF9BCcTnYWlflE7UcDfaZRLmVn7dX0481o9fk2qvBuerRBF0GX1ncDHrkIpfa6vQe6OHyMOc-Df2djvJsUL0XvEV6wcsrcnxNaszclyv-nYRsGv-dPZ0bgfF3cUGqz-X3AndlwEBeeWpL0N_B2ubgNlETheXcYgrBgRw==&uniplatform=NZKPT&language=CHS"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>CRTrack: Learning Correlation-Refine network for visual object tracking. <b>Pattern Recognition</b>, 2024. [<a href="https://www.sciencedirect.com/science/article/pii/S0031320324003339"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>Selective Depth Attention Networks for Adaptive Multi-scale Feature Representation. <b>IEEE TAI</b>, 2024. [<a href="https://arxiv.org/abs/2209.10327"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>Self-supervised learning for RGB-D object tracking. <b>Pattern Recognition</b>, 2024. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324002942"><font color=blue>paper</font></a>]</p> 
</li>
<li><p>Perceiving Actions via Temporal Video Frame Pairs. <b>ACM TIST</b>, 2024. [<a href="https://dl.acm.org/doi/abs/10.1145/3652611"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Feature enhancement and coarse-to-fine detection for RGB-D tracking. <b>PRL</b>, 2024. [<a href="https://www.sciencedirect.com/science/article/pii/S0167865524000412"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Learning Adaptive Spatio-Temporal Inference Transformer for Coarse-to-Fine Animal Visual Tracking: Algorithm and Benchmark. <b>IJCV</b>, 2024. [<a href="https://link.springer.com/article/10.1007/s11263-024-02008-8"><font color=blue>paper</font></a>] [<a href="https://github.com/XU-TIANYANG/ZOO145"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Unified Referring Expression Generation for Bounding Boxes and Segmentations. <b>IEEE SPL</b>, 2024. [<a href="https://ieeexplore.ieee.org/abstract/document/10426798"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Memory Prompt for Spatio-Temporal Transformer Visual Object Tracking. <b>IEEE TAI</b>, 2024. [<a href="https://ieeexplore.ieee.org/abstract/document/10400189"><font color=blue>paper</font></a>]</p>
</li>
<li><p>UniMod1K: Towards a More Universal Large-Scale Dataset and Benchmark for Multi-Modal Learning. <b>IJCV</b>, 2024. [<a href="https://link.springer.com/article/10.1007/s11263-024-01999-8"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Generative-based Fusion Mechanism for Multi-Modal Tracking. <b>AAAI</b>, 2024. [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28325/28639"><font color=blue>paper</font></a>]</p>
</li>
<li><p>SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition. <b>AAAI</b>, 2024. [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28409/28799"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Pluggable Attack for Visual Object Tracking. <b>IEEE TIFS</b>, 2023. [<a href="http://ieeexplore.ieee.org/document/10316262/"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Scene adaptive mechanism for action recognition. <b>CVIU</b>, 2023. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314223002345"><font color=blue>paper</font></a>]</p>
</li>
<li><p>A Spatio-Temporal Robust Tracker with Spatial-Channel Transformer and Jitter Suppression. <b>IJCV</b>, 2023. [<a href="https://link.springer.com/article/10.1007/s11263-023-01902-x"><font color=blue>paper</font></a>]</p>
</li>
<li><p>GuideFuse: A novel guided auto-encoder fusion network for infrared and visible images. <b>IEEE TIM</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10330731"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Exploring Fusion Strategies for Accurate RGBT Visual Object Tracking. <b>Information Fusion</b>, 2023. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253523001975"><font color=blue>paper</font></a>]</p>
</li> 
<li><p>TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network. <b>IEEE TIP</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10122870"><font color=blue>paper</font></a>]</p>
</li> 
<li><p>LRRNet: A novel Representation Learning Guided Fusion Network for Infrared and Visible Images. <b>IEEE TPAMI</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10105495"><font color=blue>paper</font></a>] [<a href="https://github.com/hli1221/imagefusion-LRRNet"><font color=blue>Code</font></a>]</p>
</li> 
<li><p>Fast Self-guided Multi-view Subspace Clustering. <b>IEEE TIP</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10087316"><font color=blue>paper</font></a>]</p>
</li> 
<li><p>RGBD1K: A Large-scale Dataset and Benchmark for RGB-D Object Tracking. <b>AAAI</b>, 2023. (<font color=blue><b>oral</b></font>) [<a href="https://arxiv.org/pdf/2208.09787.pdf"><font color=blue>paper</font></a>] [<a href="https://github.com/xuefeng-zhu5/RGBD1K"><font color=blue>Code</font></a>] [<a href="https://github.com/XU-TIANYANG/cakes/raw/master/AAAI2023POSTER.pdf"><font color=blue>Slide</font></a>]</p>
</li> 
<li><p>Riemannian Local Mechanism for SPD Neural Networks. <b>AAAI</b>, 2023. [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25867/25639"><font color=blue>paper</font></a>] [<a href="https://github.com/XU-TIANYANG/cakes/raw/master/AAAI23_POSTER_Submanifolds.pdf"><font color=blue>Slide</font></a>]</p>
</li> 
<li><p>Towards Robust Visual Object Tracking with Independent Target-Agnostic Detection and Effective Siamese Cross-Task Interaction. <b>IEEE TIP</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10053655"><font color=blue>paper</font></a>]</p>
</li> 
<li><p>MUFusion: A general unsupervised image fusion network based on memory unit. <b>Information Fusion</b>, 2023. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522002202"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/MUFusion"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Enhanced robust spatial feature selection and correlation filter learning for UAV tracking. <b>Neural Networks</b>, 2023. [<a href="https://github.com/HonglinChu/EFSCF"><font color=blue>Code</font></a>]</p>
</li> 
<li><p>多尺度分解和八度卷积相结合的红外与可见光图像融合. <b>中国图象图形学报</b>, 2023. [<a href="http://www.cjig.cn/html/jig/2023/1/20230111.htm"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Motion Complement and Temporal Multifocusing for Skeleton-based Action Recognition. <b>IEEE TCSVT</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10015806"><font color=blue>paper</font></a>] [<a href="https://github.com/cong-wu/MCMT-Net"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Discriminative Dictionary Pair Learning With Scale-Constrained Structured Representation for Image Classification. <b>IEEE TNNLS</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9997189"><font color=blue>paper</font></a>]</p>
</li>
<li><p>U-SPDNet: An SPD manifold learning-based neural network for visual classification. <b>Neural Networks</b>, 2022. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608022004713"><font color=blue>paper</font></a>] [<a href="https://github.com/GitWR/U-SPDNet"><font color=blue>Code</font></a>]</p>
</li>
<li><p>I Know How You Move: Explicit Motion Estimation for Human Action Recognition. <b>IEEE TMM</b>, 2022. [<a href="https://ieeexplore.ieee.org/abstract/document/9907887/"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Memory-Token Transformer for Unsupervised Video Anomaly Detection. <b>ICPR</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9956318/"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Deep Metric Learning on the SPD Manifold for Image Set Classification. <b>IEEE TCSVT</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9828488"><font color=blue>paper</font></a>] [<a href="https://github.com/GitWR/SMDML"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Learning a discriminative SPD manifold neural network for image set classification. <b>Neural Networks</b>, 2022. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608022000909"><font color=blue>paper</font></a>] [<a href="https://github.com/GitWR/SMTNet"><font color=blue>Code</font></a>]</p>
</li>
<li><p>用于骨架行为识别的多维特征嵌合注意力机制. <b>中国图象图形学报</b>, 2022. [<a href="http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=20220807"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Distillation, Ensemble and Selection for building a Better and Faster Siamese based Tracker. <b>IEEE TCSVT</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9781254/"><font color=blue>paper</font></a>]</p>
</li>
<li><p>WATCH: Two-stage Discrete Cross-media Hashing. <b>IEEE TKDE</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9735350/"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Two-Stage Supervised Discrete Hashing for Cross-Modal Retrieval. <b>IEEE TSMC</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9715729"><font color=blue>paper</font></a>]</p>
</li>
<li><p>DreamNet: A Deep Riemannian Manifold Network for SPD Matrix Learning. <b>ACCV</b>, 2022. [<a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Wang_DreamNet_A_Deep_Riemannian_Manifold_Network_for_SPD_Matrix_Learning_ACCV_2022_paper.pdf"><font color=blue>paper</font></a>] [<a href="https://github.com/GitWR/DreamNet"><font color=blue>Code</font></a>]</p>
</li>
<li><p>KITPose: Keypoint-Interactive Transformer for Animal Pose Estimation. <b>PRCV</b>, 2022. (<font color=blue><b>oral, best student paper award</b></font>) [<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-18907-4_51.pdf"><font color=blue>paper</font></a>] [<a href="https://github.com/Raojiyong/KITPose"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Adaptive channel selection for robust visual object tracking with discriminative correlation filters. <b>IJCV</b>, 2021. [<a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01435-1.pdf"><font color=blue>paper</font></a>] [<a href="https://github.com/XU-TIANYANG/ACSDCF"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Locality-constrained collaborative representation with multi-resolution dictionary for face recognition. <b>PRCV</b>, 2021. [<a href="https://link.springer.com/content/pdf/10.1007/978-3-030-88004-0_5.pdf"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Unifusion: A lightweight unified image fusion network. <b>IEEE TIM</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9550760"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/UNIFusion"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Hybrid riemannian graph-embedding metric learning for image set classification. <b>IEEE TBD</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9540380"><font color=blue>paper</font></a>]</p>
</li>
<li><p>FEXNet: Foreground Extraction Network for Human Action Recognition. <b>IEEE TCSVT</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9509412"><font color=blue>paper</font></a>]</p>
</li>
<li><p>MSC-Fuse: An Unsupervised Multi-scale Convolutional Fusion Framework for Infrared and Visible Image. <b>ICIG</b>, 2021. [<a href="https://link.springer.com/content/pdf/10.1007/978-3-030-87355-4_4.pdf"><font color=blue>paper</font></a>]</p>
</li>
<li><p>From RGB to depth: domain transfer network for face anti-spoofing. <b>IEEE TIFS</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9507460"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Dah: Discrete asymmetric hashing for efficient cross-media retrieval. <b>IEEE TKDE</b>, 2022. [<a href="https://ieeexplore.ieee.org/document/9495244"><font color=blue>paper</font></a>] [<a href="https://github.com/zhangdlin/DAH"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Adaptive feature fusion for visual object tracking. <b>Pattern Recognition</b>, 2021. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320304829"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Robust visual object tracking via adaptive attribute-aware discriminative correlation filters. <b>IEEE TMM</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9318537"><font color=blue>paper</font></a>]</p>
</li>
<li><p>An accelerated correlation filter tracker. <b>Pattern Recognition</b>, 2020. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319304728"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Complementary discriminative correlation filters based on collaborative representation for visual object tracking. <b>IEEE TCSVT</b>, 2020. [<a href="https://ieeexplore.ieee.org/document/9028150"><font color=blue>paper</font></a>]</p>
</li>
<li><p>Learning low-rank and sparse discriminative correlation filters for coarse-to-fine visual object tracking. <b>IEEE TCSVT</b>, 2020. [<a href="https://ieeexplore.ieee.org/document/8854808"><font color=blue>paper</font></a>] [<a href="https://github.com/XU-TIANYANG/LSDCF"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Learning Adaptive Discriminative Correlation Filters via Temporal Consistency preserving Spatial Feature Selection for Robust Visual Object Tracking. <b>IEEE TIP</b>, 2019. [<a href="https://ieeexplore.ieee.org/document/8728173/"><font color=blue>paper</font></a>] [<a href="https://github.com/XU-TIANYANG/LADCF"><font color=blue>Code</font></a>]</p>
</li>
<li><p>Joint group feature selection and discriminative filter learning for robust visual object tracking. <b>ICCV</b>, 2019. [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Joint_Group_Feature_Selection_and_Discriminative_Filter_Learning_for_Robust_ICCV_2019_paper.pdf"><font color=blue>paper</font></a>] [<a href="https://github.com/XU-TIANYANG/GFS-DCF"><font color=blue>Code</font></a>] [<a href="https://github.com/XU-TIANYANG/cakes/raw/master/ICCV2019POSTER.pdf"><font color=blue>Slide</font></a>]</p>
</li>
<li><p>Non-negative subspace representation learning scheme for correlation filter based tracking. <b>ICPR</b>, 2018. [<a href="https://ieeexplore.ieee.org/document/8546146"><font color=blue>paper</font></a>]</p>
</ul>
<h3><font color=blue> Awards </font></h3>
<ul>
<li><p><b>2nd-Place Award</b>,  Perception Test Challenge - Temporal Sound Localisation (ECCV2024) </p> 
</li>
<li><p><b>1st-Place Award</b>, MMVRAC 2024 - Animal Pose Estimation (ICME2024) </p> 
</li>
<li><p><b>2nd-Place Award</b>, MMVRAC 2024 - Person Reidentification (ICME2024) </p>
</li>
<li><p><b>2nd-Place Award</b>, MMVRAC 2024 - Skeleton-based Action Recognition (ICME2024) </p>
</li>
<li><p><b>2nd-Place Award</b>, Perception Test Challenge - Point Tracking (ICCV2023) </p>
</li>
<li><p><b>Most Novel Submissions</b>, Perception Test Challenge (ICCV2023) </p>
</li>
<li><p><b>2nd-Place Award</b>, AI City Challenge 2023 - Naturalistic Driving Action Recognition (CVPR2023) </p>
</li>
<li><p>PRCV 2022 <b>Best Student Paper Award</b></p>
</li>
<li><p><b>2nd-Place Award</b>, VOT-LT2022 Challenge (ECCV2022) </p>
</li>
<li><p><b>2nd-Place Award</b>, VOT-D2022 Challenge (ECCV2022)</p>
</li>
<li><p>ICCPR 2022 <b>Best Presentation Award</b></p>
</li>
<li><p><b>3rd-Place Award</b>, Make-up Dense Video Captioning Challenge (ACM MM2022)</p>
</li>
<li><p><b>2021年度中国图象图形学学会优秀博士学位论文奖</b></p>
</li>
<li><p><b>1st-Place Award</b>, Multi-Modal Video Reasoning and Analyzing Competition 2021- Skeleton-based Action Recognition (ICCV2021)</p>
</li>
<li><p><b>1st-Place Award</b>, Multi-Modal Video Reasoning and Analyzing Competition 2021- Fisheye Video-based Action Recognition (ICCV2021)</p>
</li>
<li><p>IEEE CCIS 2021 <b>Best Paper Award</b></p>
</li>
<li><p><b>2nd-Place Award</b>,  VOT-RGBD2021 Challenge (ICCV2021)</p>
</li>
<li><p><b>3rd-Place Award</b>,  Anti-UAV2021 Challenge (CVPR2021)</p>
</li>
<li><p><b>1st-Place Award</b>,  VOT-RGBT2020 Challenge (ECCV2020)</p>
</li>
<li><p><b>1st-Place Award</b>,  Anti-UAV2020 Challenge (CVPR2020)</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
  
